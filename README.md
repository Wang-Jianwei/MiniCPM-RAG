# MiniCPM 桌面智能助手开发设计文档 (v1.0)

## 1. 项目概述

本项目旨在开发一个基于 MiniCPM 系列轻量化大模型的桌面智能助手。初期实现终端交互版本，支持本地私有知识库检索（RAG），具备完全离线、低延迟、保护隐私的特点，为后续集成至 GUI 桌面应用奠定基础。

## 2. 开发需求分析

### 2.1 核心功能需求

本地模型加载：支持加载 GGUF 格式的 MiniCPM 量化模型（如 INT4）。
流式对话交互：在终端实现类似 ChatGPT 的逐字输出效果。
本地知识库集成（RAG）：支持读取本地文本文件（.txt/.md），根据用户提问自动匹配相关背景知识。
多轮对话管理：助手需具备一定的上下文记忆能力（设定 Token 窗口上限）。
性能监控：实时显示模型推理速度（tokens/s）和内存占用。

### 2.2 非功能需求

跨平台兼容性：代码需在 Windows 环境下通过 MinGW 编译器无缝编译。
轻量化：运行时显存/内存占用控制在 2GB-4GB 之间。
响应速度：首字生成延迟（First Token Latency）需控制在 2 秒以内。

## 3. 工具与环境要求

### 3.1 编译器与构建工具

编译器：MinGW-w64 (建议版本 GCC 13.1 或以上)，需支持 POSIX 线程模型。
构建工具：CMake (3.20+)。
版本控制：Git。

### 3.2 硬件建议

CPU：支持 AVX2 指令集的现代处理器（如 Intel 10代/AMD Zen2 及以后）。
内存：至少 8GB RAM。
存储：预留 5GB 以上磁盘空间（用于存放模型文件）。

## 4. 第三方库要求

### 4.1 推理核心：llama.cpp

作用：作为底层推理引擎，负责加载 MiniCPM 模型并执行张量计算。
原因：原生支持 C/C++，针对 CPU（AVX2/AVX512）有极致优化，且完美适配 MinGW。
集成方式：以 Git Submodule 形式引入，链接生成的 llama 和 common 静态库。

### 4.2 向量检索（可选）：Faiss-light 或简单相似度算法

作用：实现知识库的语义搜索。
实现方式：
方案 A（初级）：使用简单的关键词匹配或 BM25 算法（纯 C++ 实现）。
方案 B（进阶）：集成 Faiss 的轻量化版本，用于管理文本 Embedding。

### 4.3 字符编码：iconv (MinGW 自带)

作用：处理 Windows 终端（GBK）与大模型内部（UTF-8）之间的字符转换，防止中文乱码。

## 5. 开发流程规划

第一阶段：环境搭建与基准测试
配置 MinGW 与 CMake 环境变量。
克隆并编译 llama.cpp 示例工程，确保硬件加速（AVX2）正常开启。
下载 MiniCPM-2B-Q4_K_M.gguf 模型，验证基础推理能力。
第二阶段：核心推理模块开发
初始化模块：封装模型加载、上下文（Context）创建、Token 限制设定。
生成逻辑：实现循环预测（Sampling）逻辑，支持流式回调函数。
对话模版：针对 MiniCPM 的 Prompt 模版（如 <用户>...<AI>）进行适配。
第三阶段：本地知识库（RAG）实现
数据解析：编写文本分段器（Text Splitter），将说明书切分为固定长度的块。
检索器开发：构建简单的本地索引，根据用户输入的关键词抽取相关段落。
Prompt 注入：设计增强 Prompt，将检索到的“事实”动态插入到发送给模型的提示词中。
第四阶段：终端交互系统
实现支持中文输入的循环控制台。
添加特殊指令（如 /clear 清除记忆，/load 加载新知识库）。
优化输出格式，添加色彩高亮显示。

## 6. 模型规格说明

模型参数	格式	大小	适用场景
MiniCPM-2B-Text	GGUF (Q4_K)	~1.3 GB	纯文本对话、FAQ 助手、低性能电脑
MiniCPM-V-2.6	GGUF (Q4_K)	~2.5 GB	需要识别 UI 截图或处理图像输入时

## 7. 风险与注意事项

内存溢出：在 MinGW 环境下，如果 n_ctx（上下文长度）设置过大，可能会导致申请内存失败。
编码问题：Windows 终端默认不是 UTF-8，开发时需显式调用 SetConsoleOutputCP(65001)。
性能瓶颈：若无独立显卡，应重点优化 CPU 指令集配置（CMake 开启 -mavx2）。
